---
title: 'Chapter 5: Area of Applicability'
author: "Marvin Ludwig"
format: html
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: console
---


```{r setup}
#| warning: false
#| message: false
#| code-fold: true
#| code-summary: "Code: packages"


# packages:
## for data handling
library(sf)
library(terra)
library(dplyr)

## for modelling
library(caret)
library(ranger)
library(CAST)

## for visualizations
library(ggplot2)
library(ggExtra)
library(tmap)
library(viridis)

## for utility
library(here)


normalize = function(x){
  return((x-min(x))/(max(x)-min(x)))
}
```




```{r readdata}
#| code-fold: true
#| code-summary: "Code: read data"

plots = st_read("data/plots.gpkg", quiet = TRUE)
predictors = rast("data/predictors.tif")
modeldomain = st_read("data/modeldomain.gpkg", quiet = TRUE)
predictor_names = names(predictors)
response_name = "Species_richness"

# models
rcv_model = readRDS("modelling/randomcv_model.RDS")
scv_model = readRDS("modelling/spatialcv_model.RDS")
ffs_model = readRDS("modelling/ffs_model.RDS")
```


## What is the feature space?


Simplified we can think of the featurespace as the "range" of predictors we have.
Think about a case where we have only two predictors: bio_2 and bio_15. We plot each (normalized) predictor on one axis of a scatterplot. This is now our __featurespace of the whole target area__ we want to predict - in this case, Europe.
Depending on what predictors we choose (i.e. how we represent the environment) the featurespace looks different. And in reality, the feature space has as many dimensions as the number of predictors.

```{r featurespace-targetarea}
#| code-fold: true
#| code-summary: "Code: feature space"

predictors_simple = predictors[[c("bio_1", "bio_12")]] |> as.data.frame()

# normalize both predictors between 0 and 1
#predictors_simple$bio_1 = normalize(predictors_simple$bio_1)
#predictors_simple$bio_12 = normalize(predictors_simple$bio_12)


# plot each predictor on one axis, next to the scatterplot is the density of each predictor
p = ggplot(predictors_simple, aes(x = bio_1, y = bio_12))+
  geom_point(size = 0.5, alpha = 0.5, shape = 15)+
  theme_bw()

ggExtra::ggMarginal(p, type = "density")
```


Our reference samples we use for the model training now contain a subset of this featurespace.
And depending on the sampling design (random or clustered) we cover different parts of the feature space better or worse.
Here we plot the __feature space of the reference samples__ along with the feature space of the entire target area. 


```{r featurespace-reference}
#| code-fold: true
#| code-summary: "Code: feature space with samples"

# create new column to indicate different groups for coloring
predictors_simple$sampletype = "predictorspace"

# create one long dataframe that contains all the points we want to plot
pts_simple = rbind(
predictors_simple,
data.frame(bio_1 = (plots$bio_1),
           bio_12 = (plots$bio_12),
           sampletype = "reference_samples")
)

# factor for better order and visibility in the plot
pts_simple$sampletype = factor(pts_simple$sampletype, levels = c("predictorspace", "reference_samples"))
  

p2 = ggplot(pts_simple, aes(bio_1, bio_12, color = sampletype, alpha = sampletype,
                            size = sampletype, shape = sampletype))+
  geom_point()+
  scale_color_manual(values = c("black", "red"))+
  scale_alpha_manual(values = c(0.2, 1, 1))+
  scale_size_manual(values = c(0.5, 1, 1))+
  scale_shape_manual(values = c(15, 3, 4))+
  theme_bw()+
  theme(legend.position = "bottom")

ggExtra::ggMarginal(p2, groupColour = TRUE, type = "density")
```

Our goal is now to figure out which areas (i.e. pixel in the predictor raster) are represented in our reference samples in terms of their featurespace. __Only in these areas a machine learning model can be validly applied.__
For this, we use the `aoa` function in `CAST`.


## Dissimilarity Index

The calculation of the Dissimilarity Index (DI) is part if the `aoa` function and very straight forward. In its simplest form, all we need are the predictor data and the extracted reference sample data.frame. We can specify which layers of the raster and which columns of the data.frame the function should use by providing a `variables` argument. Here we use all the available predictors again.


```{r DI}
AOA = CAST::aoa(newdata = predictors,
                train = plots |> st_drop_geometry(),
                variables = predictor_names)
```


The function will notify us about some things: 

  1. the computation of a trainDI,
  2. that we provided no weights, and
  3. that we provided no model, no CV folds and that the DI threshold is therefore based on all training data.
  
We will clarify these notifications later on and see what options we have here. But first let's have a look at the DI. You can notice low DIs around our reference samples and higher DIs in areas without samples. Some regions in the Andes even have very high DIs which indicates that these environments are not represented in the training samples.


```{r DI-vis}
#| layout-ncol: 1
#| warning: false
#| code-fold: true
#| code-summary: "Code: DI Map"
tm_shape(AOA$DI)+
  tm_raster(style = "cont",
            palette = mako(50, direction = -1),
            breaks = seq(0,2,0.5),
            legend.reverse = TRUE)+
  tm_shape(plots)+
  tm_dots()+
  tm_layout(legend.position = c("right", "bottom"),
            legend.just = "right",
            frame = FALSE)
```





## Area of Applicability

What can we do now with the Dissimilarity Index? The `aoa` function also computed a second raster layer: the Area of Applicability (AOA). The AOA is a threshold based classification of the DI and indicates if the featurespace of a pixel is too dissimilar to the feature space of the reference samples (AOA = 0). We consider prediction outside the AOA as invalid.



```{r AOA}
#| warning: false
#| layout-ncol: 1
#| code-fold: true
#| code-summary: "Code: AOA Map"
tm_shape(AOA$AOA)+
  tm_raster(style = "cat", palette = c("darkgoldenrod2", "grey90"))+
  tm_shape(plots)+
  tm_dots()+
    tm_layout(legend.position = c("right", "bottom"),
            legend.just = "right",
            frame = FALSE)
```



If you plot the raw output of the `aoa` function, you will get a figure that indicates:

1. the density distribution of DIs between the reference samples (trainDI)
2. the density distribution of DIs the new locations (predictionDI)
3. the DI threshold where we consider a pixel outside the AOA

```{r}
#| warning: false
plot(AOA)
```


But how is this threshold calculated? In this simple case (without a model and cross-validation, as noted by the message above) the threshold is the outlier removed maximum dissimilarity between all reference sample points (see Meyer et al. 2021). The DI between references samples is also stored along with other important information in the object we got back from the `aoa` function, called `parameters`.



```{r}
AOA$parameters
names(AOA$parameters)
# The DI threshold is the upper whisker of a boxplot
# of the DI between training samples
boxplot(AOA$parameters$trainDI, horizontal = TRUE)
```

This `parameters` object is the output of the function `trainDI`. Hence, we can compute the information related to the reference samples separately from the computation of the AOA for a prediction area. And later use the created `parameters` in the `aoa` function (This is the first message from the `aoa` function above). This can save a lot of time and resources if you want to test different sampling strategies, models or predictors and their influence on the DI and enables paralellization options (see CAST vignette).



:::{.callout-note}
On the R help page of `trainDI` you can find an explanation for all the output list entries.
:::



```{r}
# compute dissimilarity between training samples (for DI threshold)
tdi = CAST::trainDI(train = plots |> st_drop_geometry(),
                    variables = predictor_names)
tdi

# use the trainDI object in the aoa function
AOA = CAST::aoa(newdata = predictors,
                trainDI = tdi)
```


### AOA of a model

Until now, we did all examples with the reference points and predictors only. However, the Area of Applicability was developed to enhance the validation of predictions of a spatial prediction model. Hence, the true power of the AOA is demonstrated when we use it in conjunction with a machine learning model and the associated cross-validation strategy that is used to evaluate the model. So first of all, lets build a model and evaluate it via cross-validation.

```{r}
set.seed(123)
# random forest, clustered sample, random CV
rf_clustered_rcv <- train(y = plots$Species_richness,
                          x = st_drop_geometry(plots) |>
                            dplyr::select(all_of(predictor_names)),
                          method="ranger", 
                          trControl = trainControl(method = "cv",
                                                   number = 10,
                                                   savePredictions = TRUE),
                          tuneGrid = data.frame(mtry = 5,
                                                min.node.size = 5,
                                                splitrule = "variance"),
                          importance = "permutation")
rf_clustered_rcv
```

:::{.callout-note}
## Setting the right parameters
The `aoa` and `calibrate_aoa` function require certain information form the model object that are not computed by default.
First, setting the `importance` argument gets us the variable importance in the random forest model from the ranger package. Additionally in `trainControl` we have to set `savePredictions = TRUE` in order to keep the outputs of the individual cross-validation predictions.
:::



If we have a model, we can now simply use it in the `aoa` function. The reference samples are no longer necessary since they are stored in the output of `caret::train`.

```{r}
AOA_rf_clustered_rcv = aoa(newdata = predictors, model = rf_clustered_rcv)
```

Notice that the messages about weightings and the DI threshold are gone. Since we specified `importance = "permutation"` in the `train` function, out model output contains the variable importance. We see that some variables are more important than others and we want to use this information in the computation of the dissimilarity index. If a variable is less important, it should have less impact in the calculation of the DI. Hence, before the calculation of the DI, predictors are weighted based on their importance in the model.

```{r}
# predictor importance in the model
plot(varImp(rf_clustered_rcv))

# predictor weights in the calculation of the DI
AOA_rf_clustered_rcv$parameters$weight
```

Look at the difference the weighting made:

```{r}
#| layout-ncol: 2
#| code-fold: true

plot(AOA_rf_clustered_rcv$AOA, main = "AOA: clustered samples, weighted with variable importance")
plot(AOA_clustered$AOA, main = "AOA: clustered samples, unweighted")
```


The computation of the trainDI now takes the cross-validation into account. Instead of computing the dissimilarity between all reference samples, we now compute the dissimilarity between samples in different cross-validation folds.

```{r}
plot(AOA_rf_clustered_rcv) + ggtitle("Weighted")
plot(AOA_clustered) + ggtitle("Unweighted")
```


:::{.callout-important}
## AOA Interpretation
Predictions inside the AOA are based on predictor values that are similar enough to those in the training samples. The chosen cross-validation strategy tested the model performance on reference samples with a certain dissimilarity index. Hence, predictions on pixel that are more dissimilar than the maximum dissimilarity we observed during cross-validation are not evaluated. We cannot make an assumption about the prediction quality here and mask the prediction. The pixel is outside the AOA. 
:::

Based on this, different cross-validation strategies lead to different AOAs.

```{r}
# compare knndm AOA with random AOA
```


## Calibrate AOA


We can "translate" the DI to an expected error for a pixel based estimation of the map quality.

```{r}
calib_rf_clustered = calibrate_aoa(AOA_rf_clustered_rcv, model = rf_clustered_rcv)
```


Applying this model to the DI gives us a map of expected error. Of course, some areas are still to dissimilar for a meaningful prediction and get masked of by the AOA.


### Expected Error from the model

```{r}
#| code-fold: true
#| code-summary: "Code: Map expected error"
#| warning: false
#| message: false
tm_shape(calib_rf_clustered$AOA$expected_RMSE)+
  tm_raster(title = "Expected RMSE",
            style = "cont",
            palette = mako(50, direction = -1),
            breaks = seq(0,0.1,0.02),
            legend.is.portrait = FALSE)+
  tm_shape(calib_rf_clustered$AOA$AOA)+
  tm_raster(style = "cat", palette = c("darkgoldenrod2", NA))+
  tm_shape(pts_clustered)+
  tm_dots()
```

### Expected Error from multiple cross-validations 


This might take a while!


```{r}
calib_rf_clustered_multi = calibrate_aoa(AOA_rf_clustered_rcv,
                                         model = rf_clustered_rcv,
                                         multiCV = TRUE,
                                         length.out = 4)
```


Because we tested our model by cross-validation with much more dissimilar folds (by clustering them in feature space), we can assume that our expected error is valid for more dissimilar places in our prediction area. Areas that were previously outside the AOA are now considered inside the AOA, but with a higher estimated error. We can extend the color scale of the expected error in order to make this more visible.


```{r}
#| layout-ncol: 2
#| code-fold: true
#| code-summary: "Code: Map expected error, multi CV"
#| warning: false
#| message: false



tm_shape(calib_rf_clustered_multi$AOA$expected_RMSE)+
  tm_raster(title = "Expected RMSE",
            style = "cont",
            palette = mako(50, direction = -1),
            breaks = seq(0,0.1,0.02),
            legend.is.portrait = FALSE)+
  tm_shape(calib_rf_clustered_multi$AOA$AOA)+
  tm_raster(style = "cat", palette = c("darkgoldenrod2", NA))+
  tm_shape(pts_clustered)+
  tm_dots()



tm_shape(calib_rf_clustered_multi$AOA$expected_RMSE)+
  tm_raster(title = "Expected RMSE",
            style = "cont",
            palette = mako(50, direction = -1),
            breaks = seq(0,0.3,0.1),
            legend.is.portrait = FALSE)+
  tm_shape(calib_rf_clustered_multi$AOA$AOA)+
  tm_raster(style = "cat", palette = c("darkgoldenrod2", NA))+
  tm_shape(pts_clustered)+
  tm_dots()

```






