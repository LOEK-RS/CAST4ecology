---
title: "Example data set"
output: html_document
date: "2022-10-31"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,  message = FALSE, warning = FALSE,
                      fig.align = 'center')
```


```{r}

#' Generation of a random mathematical expression
#' @description 
#' This function generates a response based on a random expression to combine the predictors of a given stack to simulate a target variable.
#' @param raster SpatRaster
#' @param predictornames string, names of the predictors that the response should be based on
#' @param seed numeric
#' @return SpatRaster with the response


generate_random_response <- function(raster, predictornames = names(raster), seed = sample(seq(1000), 1)){
  operands_1 = c("+", "-", "*", "/")
  operands_2 = c("^1","^2")
  
  expression <- paste(as.character(predictornames, sep=""))
  # assign random power to predictors
  set.seed(seed)
  expression <- paste(expression,
                      sample(operands_2, length(predictornames), replace = TRUE),
                      sep = "")
  
  # assign random math function between predictors (expect after the last one)
  set.seed(seed)
  expression[-length(expression)] <- paste(expression[-length(expression)],
                                           sample(operands_1, length(predictornames)-1, replace = TRUE),
                                           sep = " ")
  print(paste0(expression, collapse = " "))
  # collapse
  e = paste0("raster$", expression, collapse = " ")
  
  response = eval(parse(text = e))
  
  
  return(response)
  
}


```



In this script, the data used in the book chapter will be created:
predictors based on worldclim, a virtual habitat suitability for a virtual species as response,
two different sets of simulated training data, and randomly sampled validation data points.

# Getting started
```{r libraries}
library(terra)
library(sf)
```

# Simulation of the training set

Let's first define settings for the prediction task, like the number of training points, their distribution (in terms of clusters) as well as parameters on how to simulate the response.


## Get data

To prepare the predictors, the bio-climate data are downloaded and cropped to the defined study area.

```{r data}
# download, select and crop predictors:
predictors <- rast("data/worldclim.tif")
```

## Generate Predictors and Response

The virtual response variable is created based on the PCA of a subset of the bioclim predictors. See the virtualspecies package for further information.

```{r settings}
#### Simulated predictors and response:
predictornames <- c("bio1","bio2","bio5","bio6","bio7","bio10","bio11","bio12","bio13","bio14","bio15","bio18","bio19")
simulateResponse <- c("bio2","bio5","bio10", "bio13", "bio14","bio19") # variables used to simulate the response



response = generate_random_response(raster = predictors, predictornames = simulateResponse, seed = 81)
plot(response)
```

```{r predictors, fig.width=9, fig.height=6}
predictors <- predictors[[predictornames]]
coords <- crds(predictors,df=TRUE,na.rm=FALSE)
predictors$lat <- predictors[[1]]
values(predictors$lat) <- coords[,1]
predictors$lon <- predictors[[1]]
values(predictors$lon) <- coords[,2]
plot(predictors)
writeRaster(predictors,"data/predictors.tif",overwrite=TRUE)
```

```{r variables}
response_sim <- generateSpFromPCA(as(predictors[[simulateResponse]],"Raster"),
                                 means = meansPCA,sds = sdPCA, plot=F)
response <- rast(response_vs$suitab.raster)
crs(response) <- "EPSG:4326"
names(response) <- "suitability"
writeRaster(response, "data/response.tif", overwrite=TRUE)
```

## Simulate training and testing points

When looking at typical global prediction studies, we can see that reference data that are used for model training are often extremely clustered in geographic space (see https://www.nature.com/articles/s41467-022-29838-9).
To simulate field locations that are typically used as training data, "nclusters" locations are randomly selected (center of clusters).
The "npoints" are then distributed over the clusters, with a maximum distance of "maxdist" meters around the center of each cluster. For comparison, randomly distributed training data are created as well.

```{r clusteredpoints, include=FALSE, warning=FALSE}
#For a clustered design:
# adjusted from from https://github.com/carlesmila/NNDMpaper/blob/main/code/sim_utils.R
clustered_sample <- function(sarea, nsamples, nparents, radius, seed){
  # Number of offspring per parent
  nchildren <- round((nsamples-nparents)/nparents, 0)
  # Simulate parents
  set.seed(seed)
  parents <- st_sf(geometry=st_sample(sarea, nparents, type="random"))
  res <- parents
  res$clstrID <- 1:nrow(parents)
  # Simulate offspring
  for(i in 1:nrow(parents)){
    # Generate buffer and cut parts outside of the area of study
    buf <- st_buffer(parents[i,], dist=radius)
    buf <- st_intersection(buf, sarea)
    # Simulate children
    set.seed(seed)
    children <- st_sf(geometry=st_sample(buf, nchildren, type="random"))
      children$clstrID <- i
    res <- rbind(res, children)
  }
  return(res)
}
```

```{r samplepoints_settings}
npoints <- 300 # number of training samples
nclusters <- 10 #number of clusters if design==clustered
maxdist <- 250000 #in unit m. size of the clusters
seed <- 2345
```

```{r samplepoints_mask}
# create a mask for land area
mask <- as(predictors[[1]],"Raster")
values(mask)[!is.na(values(mask))] <- 1
mask <- st_as_sf(rasterToPolygons(mask,dissolve=TRUE))
st_crs(mask) <- 4326
```

```{r samplepoints}
# Simulate points
set.seed(seed)
pts_random <- st_sf(geometry = st_sample(mask, npoints))
pts_validation <- st_sf(geometry = st_sample(mask, npoints))
pts_clustered <- clustered_sample(mask, npoints, nclusters, radius=maxdist, seed=seed)
pts_clustered$clstrID <- NULL # We don't need it
```

```{r extraction}
# Stack and extract
rstack <- c(response, predictors)
df_random <- extract(rstack, vect(pts_random), ID=FALSE)
df_validation <- extract(rstack, vect(pts_validation), ID=FALSE)
df_clustered <- extract(rstack, vect(pts_clustered), ID=FALSE)
pts_random <- cbind(pts_random, df_random)
pts_validation <- cbind(pts_validation, df_validation)
pts_clustered <- cbind(pts_clustered, df_clustered)
```

```{r write, include=FALSE}
# Write
st_write(mask,"data/AOI.gpkg", append=FALSE)
st_write(pts_clustered,"data/pts_clustered.gpkg", append=FALSE)
st_write(pts_random,"data/pts_random.gpkg", append=FALSE)
st_write(pts_validation,"data/pts_validation.gpkg", append=FALSE)
```

Now let's visualize the created response variable and the training data points

```{r vis_data, fig.width=10, fig.height=3}
par(mfrow=c(1,3))
plot(response,main="response and clustered training data")
plot(pts_clustered,add=T, col="black",cex=0.5)
legend("topleft",pch=1,legend="training",bty="n",col="black",pt.cex=0.5)

plot(response,main="response and random training data")
plot(pts_random,add=T, col="black",cex=0.5)
legend("topleft",pch=1,legend="training",bty="n",col="black",pt.cex=0.5)

plot(response,main="response and validation data")
plot(pts_validation,add=T, col="black",cex=0.5)
legend("topleft",pch=1,legend="training",bty="n",col="black",pt.cex=0.5)
```



